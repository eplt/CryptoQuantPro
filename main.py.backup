"""CryptoQuant Pro - Main orchestration script for v0.2.0"""

from data.data_collector import DataCollector
from data_processing import (
    get_token_selection_mode, validate_manual_tokens, collect_tokens_by_mode
)
from portfolio_optimization import (
    build_portfolio_options, select_best_portfolio, test_allocation_methods,
    run_monte_carlo_optimization, apply_monte_carlo_if_better, run_backtests,
    find_best_drift_threshold
)
from reporting import (
    prompt_for_report_generation, generate_report_if_requested,
    save_results_to_json, export_detailed_csv, generate_performance_charts
)
from evaluation.token_evaluator import TokenEvaluator
from config.settings import *
from __version__ import __version__, __release__
import os
import time
from datetime import datetime, timedelta


def display_token_results(token_scores, selection_mode, manual_tokens):
    """Display token analysis results."""
    start_time = time.time()
    
    print("=== CryptoQuant Pro - Advanced Portfolio Analysis ===")
    print(f"Version {__version__} - {__release__}")
    print(f"Using {os.cpu_count()} CPU cores for parallel processing")
    print(f"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # Initialize data storage for potential report generation
    analysis_data = {}
    execution_log = []
    charts = []
    
    # Step 0: Token Selection Mode
    selection_mode, manual_tokens = get_token_selection_mode()
    
    if selection_mode != 'auto':
        print(f"\n‚úì Token selection mode: {selection_mode}")
        if manual_tokens:
            print(f"   Manual tokens: {', '.join(manual_tokens)}")
    
    # Step 1: Data Collection with Smart Caching
    print(f"\n1. Collecting data ({selection_mode} mode)...")
    data_start = time.time()
    collector = DataCollector()
    
    # Show cache status
    cache_status = collector.get_cache_status()
    if 'total_files' in cache_status:
        print(f"Cache status: {cache_status['fresh_files']} fresh, "
              f"{cache_status['old_files']} old files "
              f"({cache_status['cache_size_mb']} MB total)")
    
    # Clean old cache files
    try:
        cleaned = collector.clean_old_cache(max_age_days=30)
        if cleaned['files_removed'] > 0:
            print(f"Cleaned {cleaned['files_removed']} old cache files")
    except Exception as e:
        print(f"Cache cleaning failed: {e}")
    
    # Collect data based on selection mode
    try:
        price_data, market_data = collect_tokens_by_mode(collector, selection_mode, manual_tokens)
        data_time = time.time() - data_start
        
        print(f"‚úì Data ready for {len(price_data)} tokens in {data_time:.1f} seconds")
        
        # Validate manual tokens if applicable
        if selection_mode in ['manual', 'hybrid'] and manual_tokens:
            valid_manual = validate_manual_tokens(manual_tokens, price_data, market_data)
            if valid_manual is None:
                print("Exiting due to insufficient valid manual tokens.")
                return
            
            # Store which tokens were manually selected for reporting
            analysis_data['manual_token_selection'] = {
                'mode': selection_mode,
                'requested_tokens': manual_tokens,
                'valid_tokens': valid_manual
            }
        
        execution_log.append({
            'timestamp': datetime.now().isoformat(),
            'step': 'Data Collection',
            'details': f'Collected {len(price_data)} tokens ({selection_mode} mode)',
            'duration_seconds': data_time
        })
        
        if len(price_data) < 2:
            print("Error: Need at least 2 tokens for portfolio analysis.")
            return
            
    except Exception as e:
        print(f"‚úó Error collecting  {e}")
        return
    
    # Step 2: Token Evaluation (Parallel)
    print("\n2. Evaluating tokens with multiprocessing...")
    eval_start = time.time()
    
    try:
        evaluator = TokenEvaluator(price_data, market_data, n_cores=os.cpu_count())
        token_scores = evaluator.evaluate_all_tokens()
        
        eval_time = time.time() - eval_start
        print(f"‚úì Token evaluation completed in {eval_time:.1f} seconds")
        
        execution_log.append({
            'timestamp': datetime.now().isoformat(),
            'step': 'Token Evaluation',
            'details': f'Evaluated {len(token_scores)} tokens',
            'duration_seconds': eval_time
        })
        analysis_data['token_scores'] = token_scores
        
        if not token_scores:
            print("‚úó No tokens were successfully evaluated")
            return
        
    except Exception as e:
        print(f"‚úó Error in token evaluation: {e}")
        return
    
    # Display token results
    print(f"\nToken Analysis Results ({len(token_scores)} tokens):")
    print("Rank | Token        | Score | Volatility | Market Cap    | Liquidity")
    print("-----|--------------|-------|------------|---------------|----------")
    
    for i, (token, score_data) in enumerate(list(token_scores.items())[:min(15, len(token_scores))]):
        vol = score_data['metrics']['volatility']
        mcap = score_data['metrics']['market_cap']
        liq_ratio = score_data['metrics']['liquidity_ratio']
        
        # Highlight manually selected tokens
        marker = "üéØ" if selection_mode != 'auto' and token in manual_tokens else "  "
        
        print(f"{i+1:4d} | {token:12s} | {score_data['final_score']:5.1f} | "
              f"{vol:8.3f}   | ${mcap:11.2e} | {liq_ratio:8.6f} {marker}")
    
    if selection_mode != 'auto':
        print("üéØ = Manually selected token")
    
    # Step 3: Portfolio Construction (Parallel)
    print(f"\n3. Building optimal portfolios with parallel processing...")
    portfolio_start = time.time()
    
    try:
        builder = PortfolioBuilder(token_scores, price_data, n_cores=os.cpu_count())
        
        # Adjust max tokens based on available tokens
        max_tokens = min(10, len(token_scores))
        portfolio_options = builder.find_optimal_portfolio_size(token_scores, max_tokens)
        
        portfolio_time = time.time() - portfolio_start
        print(f"‚úì Portfolio construction completed in {portfolio_time:.1f} seconds")
        
        execution_log.append({
            'timestamp': datetime.now().isoformat(),
            'step': 'Portfolio Construction',
            'details': f'Built {len(portfolio_options)} portfolio options',
            'duration_seconds': portfolio_time
        })
        analysis_data['portfolio_options'] = portfolio_options
        
    except Exception as e:
        print(f"‚úó Error in portfolio construction: {e}")
        return
    
    if not portfolio_options:
        print("‚úó No valid portfolio options found")
        return
    
    print(f"\nPortfolio options found:")
    for size, portfolio in portfolio_options.items():
        tokens_str = ', '.join(portfolio['tokens'][:3])
        if len(portfolio['tokens']) > 3:
            tokens_str += f"... (+{len(portfolio['tokens'])-3} more)"
        print(f"  {size} tokens: {tokens_str} (Score: {portfolio['score']:.1f})")
    
    # Select best portfolio (prefer smaller portfolios for manual selection)
    if selection_mode == 'manual':
        # For manual selection, prefer portfolio size close to number of manual tokens
        target_size = min(len(manual_tokens), max(portfolio_options.keys()))
        preferred_size = target_size if target_size in portfolio_options else max(portfolio_options.keys())
    else:
        preferred_size = 4 if 4 in portfolio_options else max(portfolio_options.keys())
    
    best_portfolio = portfolio_options[preferred_size]
    print(f"\nSelected portfolio ({preferred_size} tokens): {', '.join(best_portfolio['tokens'])}")
    analysis_data['best_portfolio'] = best_portfolio
    
    # Step 4: Test Allocation Methods
    print(f"\n4. Testing allocation methods...")
    allocation_start = time.time()
    
    try:
        allocation_results = builder.test_all_allocation_methods(best_portfolio['tokens'])
        
        allocation_time = time.time() - allocation_start
        print(f"‚úì Allocation method testing completed in {allocation_time:.1f} seconds")
        
        execution_log.append({
            'timestamp': datetime.now().isoformat(),
            'step': 'Allocation Testing',
            'details': f'Tested {len(allocation_results)} methods',
            'duration_seconds': allocation_time
        })
        analysis_data['allocation_results'] = allocation_results
        
        print("\nAllocation method comparison:")
        for method, result in allocation_results.items():
            print(f"  {method:20s}: Score {result['score']:6.1f}")
            # Show top 3 allocations
            sorted_allocations = sorted(result['allocations'].items(), 
                                      key=lambda x: x[1], reverse=True)
            for token, weight in sorted_allocations[:3]:
                print(f"    {token}: {weight:.1%}")
            if len(result['allocations']) > 3:
                print(f"    ... (+{len(result['allocations'])-3} more)")
        
        # Select best allocation method
        best_allocation_method = max(allocation_results.keys(), 
                                    key=lambda k: allocation_results[k]['score'])
        best_allocations = allocation_results[best_allocation_method]['allocations']
        
        print(f"\nSelected allocation method: {best_allocation_method}")
        analysis_data['best_allocation_method'] = best_allocation_method
        analysis_data['best_allocations'] = best_allocations
        
    except Exception as e:
        print(f"‚úó Error in allocation testing: {e}")
        # Fallback to equal weight
        best_allocation_method = 'equal_weight'
        best_allocations = {token: 1/len(best_portfolio['tokens']) 
                           for token in best_portfolio['tokens']}
        print(f"Using fallback equal weight allocation")
        analysis_data['best_allocation_method'] = best_allocation_method
        analysis_data['best_allocations'] = best_allocations
    
    # Step 5: Optional Monte Carlo Optimization
    if len(best_portfolio['tokens']) <= 6:
        print(f"\n5. Running Monte Carlo optimization...")
        mc_start = time.time()
        
        try:
            mc_result = builder.parallel_monte_carlo_optimization(
                best_portfolio['tokens'], 
                n_simulations=50000
            )
            
            mc_time = time.time() - mc_start
            print(f"‚úì Monte Carlo optimization completed in {mc_time:.1f} seconds")
            print(f"  Optimal Sharpe ratio: {mc_result['sharpe_ratio']:.3f}")
            print(f"  Expected return: {mc_result['expected_return']:.2%}")
            print(f"  Expected volatility: {mc_result['expected_volatility']:.2%}")
            
            execution_log.append({
                'timestamp': datetime.now().isoformat(),
                'step': 'Monte Carlo Optimization',
                'details': '50,000 simulations completed',
                'duration_seconds': mc_time
            })
            analysis_data['monte_carlo_result'] = mc_result
            
            # Use MC-optimized allocations if significantly better
            current_score = allocation_results[best_allocation_method]['score']
            mc_score = mc_result['sharpe_ratio'] * 100  # Rough comparison
            
            if mc_result['sharpe_ratio'] > 1.0 and mc_score > current_score:
                best_allocations = mc_result['best_allocation']
                best_allocation_method = 'monte_carlo_optimized'
                print("  ‚Üí Using Monte Carlo optimized allocations")
                analysis_data['best_allocation_method'] = best_allocation_method
                analysis_data['best_allocations'] = best_allocations
            else:
                print(f"  ‚Üí Keeping {best_allocation_method} allocations")
                
        except Exception as e:
            print(f"‚úó Monte Carlo optimization failed: {e}")
    else:
        print(f"\n5. Skipping Monte Carlo (portfolio too large: {len(best_portfolio['tokens'])} tokens)")
    
    # Step 6: Backtesting with Multiple Configurations
    print(f"\n6. Running backtests with multiple configurations...")
    backtest_start = time.time()
    
    backtest_results = {}
    drift_thresholds = [0.08, 0.10, 0.12, 0.15, 0.20, 0.25]
    
    print(f"Testing {len(drift_thresholds)} drift thresholds:")
    
    successful_backtests = 0
    
    for i, drift in enumerate(drift_thresholds):
        print(f"  [{i+1}/{len(drift_thresholds)}] Testing drift threshold: {drift:.1%}...", end="")
        
        portfolio_config = {
            'tokens': best_portfolio['tokens'],
            'allocations': best_allocations,
            'drift_threshold': drift,
            'min_interval': 5,
            'max_interval': 21
        }
        
        try:
            engine = BacktestEngine(price_data, portfolio_config)
            results = engine.run_backtest(
                start_date=datetime.now() - timedelta(days=730),
                end_date=datetime.now() - timedelta(days=30),
                initial_capital=10000
            )
            backtest_results[drift] = results
            successful_backtests += 1
            print(" ‚úì")
            
        except Exception as e:
            print(f" ‚úó ({str(e)[:50]}...)")
            continue
    
    backtest_time = time.time() - backtest_start
    print(f"‚úì Backtesting completed in {backtest_time:.1f} seconds")
    print(f"  Successful backtests: {successful_backtests}/{len(drift_thresholds)}")
    
    execution_log.append({
        'timestamp': datetime.now().isoformat(),
        'step': 'Backtesting',
        'details': f'Completed {successful_backtests} backtests',
        'duration_seconds': backtest_time
    })
    analysis_data['backtest_results'] = backtest_results
    
    # Step 7: Performance Analysis
    print(f"\n7. Analyzing results...")
    
    if not backtest_results:
        print("‚úó No successful backtests to analyze")
        print("This might be due to insufficient data or configuration issues.")
        return
    
    best_drift = None
    best_sharpe = -np.inf
    best_overall_score = -np.inf
    
    print(f"\nBacktest Results Summary:")
    print("Drift   | Return | Sharpe | Max DD | Trades | Fees  | Alpha  | Score")
    print("--------|--------|--------|--------|--------|-------|--------|-------")
    
    for drift, results in backtest_results.items():
        try:
            metrics = results['performance_metrics']
            
            # Calculate combined score
            return_score = min(metrics['annualized_return'] * 100, 50)  # Cap at 50%
            sharpe_score = min(metrics['sharpe_ratio'] * 20, 40)  # Cap at 2.0 Sharpe
            drawdown_score = min((1 + metrics['max_drawdown']) * 20, 20)  # Less negative DD
            alpha_score = min(metrics['alpha'] * 100, 20)  # Cap at 20%
            fee_score = max(0, 10 - metrics['fees_pct_of_portfolio'] * 1000)  # Penalize high fees
            
            overall_score = return_score + sharpe_score + drawdown_score + alpha_score + fee_score
            
            if metrics['sharpe_ratio'] > best_sharpe:
                best_sharpe = metrics['sharpe_ratio']
                best_drift = drift
            
            if overall_score > best_overall_score:
                best_overall_score = overall_score
            
            print(f"{drift:6.1%} | {metrics['annualized_return']:5.1%} | "
                  f"{metrics['sharpe_ratio']:5.2f} | {metrics['max_drawdown']:5.1%} | "
                  f"{metrics['total_trades']:6d} | {metrics['fees_pct_of_portfolio']:4.2%} | "
                  f"{metrics['alpha']:5.1%} | {overall_score:5.1f}")
                  
        except Exception as e:
            print(f"{drift:6.1%} | ERROR: {str(e)[:50]}...")
    
    print(f"\n‚úì Best configuration: {best_drift:.1%} drift threshold (Sharpe: {best_sharpe:.3f})")
    analysis_data['best_drift'] = best_drift
    analysis_data['best_results'] = backtest_results[best_drift]
    
    # Step 8: Detailed Analysis of Best Strategy
    try:
        best_results = backtest_results[best_drift]
        analyzer = PerformanceAnalyzer(best_results)
        
        print(f"\n=== DETAILED PERFORMANCE REPORT ===")
        report = analyzer.generate_performance_report()
        print(report)
        
        # Calculate additional risk metrics
        try:
            risk_metrics = analyzer.calculate_risk_metrics()
            analysis_data['risk_metrics'] = risk_metrics
            
            print(f"\nADDITIONAL RISK METRICS:")
            print(f"- Value at Risk (5%): {risk_metrics['value_at_risk_5pct']:.3f}")
            print(f"- Conditional VaR (5%): {risk_metrics['conditional_var_5pct']:.3f}")
            print(f"- Skewness: {risk_metrics['skewness']:.3f}")
            print(f"- Kurtosis: {risk_metrics['kurtosis']:.3f}")
            print(f"- Positive Periods: {risk_metrics['positive_periods']:.1%}")
            print(f"- Max Consecutive Losses: {risk_metrics['max_consecutive_losses']} periods")
            print(f"- Calmar Ratio: {risk_metrics['calmar_ratio']:.3f}")
        except Exception as e:
            print(f"Could not calculate additional risk metrics: {e}")
        
        # Trading analysis
        try:
            trade_analysis = analyzer.generate_trade_analysis()
            if 'total_trades' in trade_analysis:
                print(f"\nTRADING ANALYSIS:")
                print(f"- Total Trades: {trade_analysis['total_trades']}")
                print(f"- Average Trade Size: ${trade_analysis['avg_trade_size']:,.0f}")
                print(f"- Largest Trade: ${trade_analysis['largest_trade']:,.0f}")
                print(f"- Buy/Sell Ratio: {trade_analysis['buy_vs_sell_ratio']:.2f}")
                print(f"- Avg Days Between Rebalances: {trade_analysis['avg_days_between_rebalances']:.1f}")
        except Exception as e:
            print(f"Could not generate trade analysis: {e}")
            
    except Exception as e:
        print(f"Error in detailed analysis: {e}")
    
    # Step 9: Save Results and Generate Charts
    print(f"\n9. Saving results and generating charts...")
    
    try:
        # Create results directory
        results_dir = 'results'
        plots_dir = 'plots'
        os.makedirs(results_dir, exist_ok=True)
        os.makedirs(plots_dir, exist_ok=True)
        
        # Save backtest results
        serializable_results = {}
        for drift, results in backtest_results.items():
            try:
                # Convert numpy types for JSON serialization
                serializable_results[str(drift)] = {
                    'performance_metrics': {
                        k: float(v) if isinstance(v, (np.integer, np.floating)) else v 
                        for k, v in results['performance_metrics'].items()
                    },
                    'portfolio_config': {
                        'tokens': list(best_portfolio['tokens']),
                        'allocations': {k: float(v) for k, v in best_allocations.items()},
                        'drift_threshold': float(drift),
                        'allocation_method': best_allocation_method,
                        'portfolio_score': float(best_portfolio['score'])
                    },
                    'execution_info': {
                        'timestamp': datetime.now().isoformat(),
                        'total_runtime_seconds': time.time() - start_time,
                        'data_age_days': cache_status.get('newest_file', {}).get('age_days', 0)
                    }
                }
            except Exception as e:
                print(f"Error serializing results for drift {drift}: {e}")
        
        results_file = os.path.join(results_dir, 'backtest_results.json')
        with open(results_file, 'w') as f:
            json.dump(serializable_results, f, indent=2)
        
        print(f"‚úì Results saved to {results_file}")
        
    except Exception as e:
        print(f"‚úó Could not save JSON results: {e}")
    
    # Save detailed results as CSV
    try:
        if best_drift in backtest_results:
            analyzer = PerformanceAnalyzer(backtest_results[best_drift])
            analyzer.export_results('detailed_results.csv')
            print(f"‚úì Detailed CSV results exported")
    except Exception as e:
        print(f"Could not export CSV results: {e}")
    
    # Generate performance charts
    save_plots = True
    if save_plots:
        try:
            chart_file = os.path.join(plots_dir, f'performance_charts_{datetime.now().strftime("%Y%m%d_%H%M")}.png')
            if best_drift in backtest_results:
                analyzer = PerformanceAnalyzer(backtest_results[best_drift])
                analyzer.plot_performance_charts(chart_file)
                print(f"‚úì Performance charts saved to {chart_file}")
                
                # Add chart for potential report generation
                charts.append({
                    'path': chart_file,
                    'caption': 'Portfolio Performance Analysis',
                    'type': 'performance'
                })
                
        except Exception as e:
            print(f"Could not generate charts: {e}")
    
    # Step 10: Final Summary (before report generation)
    total_time = time.time() - start_time
    
    print(f"\n=== ANALYSIS COMPLETE ===")
    print(f"Total execution time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)")
    print(f"  Data collection: {data_time:.1f}s")
    print(f"  Token evaluation: {eval_time:.1f}s")
    print(f"  Portfolio construction: {portfolio_time:.1f}s")
    print(f"  Allocation testing: {allocation_time:.1f}s")
    if 'mc_time' in locals():
        print(f"  Monte Carlo optimization: {mc_time:.1f}s")
    print(f"  Backtesting: {backtest_time:.1f}s")
    
    print(f"\n=== RECOMMENDED CONFIGURATION ===")
    print(f"Portfolio tokens: {', '.join(best_portfolio['tokens'])}")
    print(f"Allocation method: {best_allocation_method}")
    print(f"Optimal drift threshold: {best_drift:.1%}")
    
    if best_drift in backtest_results:
        best_metrics = backtest_results[best_drift]['performance_metrics']
        print(f"Expected performance:")
        print(f"  - Annual Return: {best_metrics['annualized_return']:.2%}")
        print(f"  - Volatility: {best_metrics['volatility']:.2%}")
        print(f"  - Sharpe Ratio: {best_metrics['sharpe_ratio']:.3f}")
        print(f"  - Max Drawdown: {best_metrics['max_drawdown']:.2%}")
        print(f"  - Alpha vs Benchmark: {best_metrics['alpha']:.2%}")
    
    print(f"\nüöÄ Core analysis finished!")
    print(f"üìÅ Results saved in: results/")
    print(f"üìä Charts saved in: plots/")
    
    # Step 11: Optional Report Generation
    report_choice = prompt_for_report_generation()
    
    if report_choice != 'none':
        generated_file = generate_report_if_requested(report_choice, analysis_data, execution_log, charts)
        
        if generated_file:
            print(f"\nüìÑ AI-enhanced PDF report: {generated_file}")
        else:
            print(f"\n‚ö†Ô∏è  Report generation failed or was skipped")
    
    # Final next steps
    print(f"\n=== NEXT STEPS ===")
    print(f"1. Review the analysis results and performance charts")
    if report_choice != 'none':
        print(f"2. Examine the comprehensive PDF report")
    print(f"3. Consider paper trading with the recommended configuration")
    print(f"4. Monitor correlation breakdowns and regime changes")
    print(f"5. Re-run analysis weekly with fresh data")
    print(f"6. Validate results with out-of-sample testing")



if __name__ == "__main__":
    # Set optimal multiprocessing settings
    os.environ['OPENBLAS_NUM_THREADS'] = '1'
    os.environ['MKL_NUM_THREADS'] = '1' 
    os.environ['NUMEXPR_NUM_THREADS'] = '1'
    os.environ['NUMBA_NUM_THREADS'] = '1'
    
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Execution interrupted by user")
    except Exception as e:
        print(f"\n\nüí• Unexpected error: {e}")
        import traceback
        traceback.print_exc()
